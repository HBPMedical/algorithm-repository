{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test: Distributed/Statistical Analysis/PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis\n",
    "\n",
    "Principal component analysis (PCA) is a standard tool in modern data analysis. The main goal of a PCA analysis is to identify patterns in data by detecting correlations between variables to reduce its dimensionality. In a nutshell, PCA finds the directions of maximum variance in high-dimensional data and project it onto a smaller dimensional subspace while retaining most of the information.\n",
    "\n",
    "PCA was invented in 1901 by Karl Pearson, as an analogue of the principal axis theorem in mechanics; it was later independently developed and named by Harold Hotelling in the 1930s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use cases\n",
    "\n",
    "The main question with PCA is how much should we reduce the dimensionality of the data. There are two ways to fixe that question:\n",
    "\n",
    "- We set a threshold (=1 in general) and we keep only dimensions for which the eigen value is obove this threshold.\n",
    "- We want to maintenant an amount of variation in the model and we keep ordered dimensions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## limitations\n",
    "\n",
    "- PCA is sensitive to the scaling of the variables. This means that whenever the different variables have different units (like temperature and mass), PCA is a somewhat arbitrary method of analysis. One way of making the PCA less arbitrary is to use variables scaled so as to have unit variance, by standardizing the data and hence use the autocorrelation matrix instead of the autocovariance matrix as a basis for PCA.\n",
    "\n",
    "- To ensure that the first principal component describes the direction of maximum variance, variables must be centered. If not, the first principal component might instead correspond more or less to the mean of the data. A mean of zero is needed for finding a basis that minimizes the mean square error of the approximation of the data.\n",
    "\n",
    "- PCA is a popular primary technique in pattern recognition, but it is not optimized for class separability. The linear discriminant analysis is an alternative which is optimized for class separability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "Download study database from the subproject: sample-data-db-setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>righthippocampus</th>\n",
       "      <th>lefthippocampus</th>\n",
       "      <th>rightamygdala</th>\n",
       "      <th>leftamygdala</th>\n",
       "      <th>rightlateralventricle</th>\n",
       "      <th>leftlateralventricle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.7933</td>\n",
       "      <td>3.4613</td>\n",
       "      <td>0.89412</td>\n",
       "      <td>0.95116</td>\n",
       "      <td>16.1951</td>\n",
       "      <td>17.7235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.5737</td>\n",
       "      <td>3.3827</td>\n",
       "      <td>0.86274</td>\n",
       "      <td>0.89655</td>\n",
       "      <td>24.7413</td>\n",
       "      <td>35.4198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.4143</td>\n",
       "      <td>3.1983</td>\n",
       "      <td>0.86853</td>\n",
       "      <td>0.89788</td>\n",
       "      <td>17.2587</td>\n",
       "      <td>19.3711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.9331</td>\n",
       "      <td>2.6429</td>\n",
       "      <td>0.68437</td>\n",
       "      <td>0.70803</td>\n",
       "      <td>42.9125</td>\n",
       "      <td>32.0855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.0757</td>\n",
       "      <td>2.8996</td>\n",
       "      <td>0.80229</td>\n",
       "      <td>0.79138</td>\n",
       "      <td>14.9264</td>\n",
       "      <td>20.9043</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   righthippocampus  lefthippocampus  rightamygdala  leftamygdala  \\\n",
       "1            3.7933           3.4613        0.89412       0.95116   \n",
       "2            3.5737           3.3827        0.86274       0.89655   \n",
       "3            3.4143           3.1983        0.86853       0.89788   \n",
       "4            2.9331           2.6429        0.68437       0.70803   \n",
       "5            3.0757           2.8996        0.80229       0.79138   \n",
       "\n",
       "   rightlateralventricle  leftlateralventricle  \n",
       "1                16.1951               17.7235  \n",
       "2                24.7413               35.4198  \n",
       "3                17.2587               19.3711  \n",
       "4                42.9125               32.0855  \n",
       "5                14.9264               20.9043  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import statsmodels.formula.api as sm\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "dataRaw = pd.read_csv('Data/desd-synthdata.csv')\n",
    "data = dataRaw[['righthippocampus', 'lefthippocampus', \n",
    "                'rightamygdala','leftamygdala',\n",
    "                'rightlateralventricle','leftlateralventricle']]\n",
    "data = data.dropna()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it seems that the method PCA standardized the data  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "data_std = StandardScaler().fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.4008611 , -0.19547397],\n",
       "       [-1.51101883,  1.98498473],\n",
       "       [-1.24524255, -0.06743021],\n",
       "       ...,\n",
       "       [-1.16294986,  2.00695644],\n",
       "       [ 0.54006644, -0.34693453],\n",
       "       [-0.70289511, -2.47390013]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "sklearn_pca = PCA(n_components= 2)\n",
    "data_transformed = sklearn_pca.fit_transform(data_std)\n",
    "data_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.60506879, 1.92461596])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenValues = sklearn_pca.explained_variance_\n",
    "eigenValues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.60506879, 1.92461596, 0.20280846, 0.12318931, 0.07951498,\n",
       "       0.07133133])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pcaFull = PCA(n_components=6)\n",
    "pcaFull.fit_transform(data_std)\n",
    "eig = pcaFull.explained_variance_\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.50039386, -0.496709  , -0.49942028, -0.49825384,  0.05034485,\n",
       "         0.0517113 ],\n",
       "       [ 0.05209064,  0.05784484,  0.0018352 ,  0.03249855,  0.70481511,\n",
       "         0.7043555 ]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eigenVectors = sklearn_pca.components_\n",
    "eigenVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEST in MIP\n",
    "### Export result\n",
    "\n",
    "Correlation matrix is computed and the results are written in a file that will be used in integration test phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "dict_export = {}\n",
    "dict_export['eigen_values'] = eigenValues.tolist()\n",
    "dict_export['eigen_vectors'] = eigenVectors.tolist()\n",
    "dict_export['data_transformed'] = data_transformed.tolist()\n",
    "\n",
    "with open('Output/PCA.json', 'w') as fp:\n",
    "    json.dump(dict_export, fp)\n",
    "#json.dump(dict_export, open('Output/PCA.json', 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods\n",
    "\n",
    "Their exist similar methods:\n",
    "\n",
    "- LDA, Linear Discriminant Analysis aims to find the directions that maximize the variance of the data and maximize the separation (or discrimination) between different classes, which can be useful in pattern classification problem.\n",
    "- "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
